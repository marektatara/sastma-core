{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook presents segmentation-aided stereo matching method with confidence refinement. This method is based on the paper \"Segmentation-aided stereo matching with confidence refinement\" by Marek S. Tatara, Jan Glinko, Michał Czubenko, Marek Grzegorek, Zdzisław Kowalczuk. The paper is available at ...\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "- Input images are already stereo rectified.\n",
    "\n",
    "## Description of the methodology\n",
    "\n",
    "The method is based on the following steps:\n",
    "1. **Segmentation**: The input images are segmented using a pre-trained segmentation model, in this case, SAM2 is used\n",
    "2. **Segment-level Stereo Matching**: The segmented fragments of the images are populated to the second image, treating the fragment as a single object to be tracked in subsequent video frames. SAM2 is used here as well\n",
    "3. **Pixel-level Stereo Matching**: For each pair of corresponding fragments, the pixel-level stereo matching is performed using the stereo matching algorithm. The proposed algorithm calculated SAD (Sum of Absolute Differences) and for a given height using the block matching approach, and for each match the confidence is calculated\n",
    "4. **Confidence Refinement**: The confidence calculated for the found matches is used to extract high-confidence matches between the left and right segments. These points are used to narrow down the search area for the remaining pixels (assuming that the disparity of a pixel must lie in the range of surrounding disparities). The confidence is calculated as the ratio of the minimum SAD to the second minimum SAD. The confidence is then used to filter out the matches with low confidence.\n",
    "5. **Global Refinement**: After all segments are matched, the global refinement is performed. The disparity of each remaining pixel is calculated using the disparity of the surrounding pixels. The disparity is calculated as the weighted average of the disparities of the surrounding pixels, where the weights are the confidence of the surrounding pixels (to be done)\n",
    "\n",
    "## References\n",
    "\n",
    "- [SAM2](https://github.com/facebookresearch/sam2)\n",
    "- [Middlebury Stereo Dataset 2021](https://vision.middlebury.edu/stereo/data/scenes2021/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run the code\n",
    "\n",
    "Install poetry and run the following commands:\n",
    "```bash\n",
    "poetry install\n",
    "```\n",
    "\n",
    "Run the code in the notebook. Everything should be downloaded and installed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base code taken from SAM2 repository, automatic mask generator connected with video predictor\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "\n",
    "from helpers import setup_device, save_annotations\n",
    "from sam2.build_sam import build_sam2_video_predictor, build_sam2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters and the device\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "OUT_PATH = \"out_images/\"\n",
    "INPUT_DATA_DIR = \"input_data/\"\n",
    "INPUT_FOLDER = \"pendulum1\"\n",
    "CHECKPOINT_PATH = \"sam2/checkpoints/\"\n",
    "\n",
    "input_dir = Path(INPUT_DATA_DIR) / \"data\" / INPUT_FOLDER\n",
    "\n",
    "Path(OUT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "Path(INPUT_DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(CHECKPOINT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "device = setup_device()\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(INPUT_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if INPUT_DATA folder is not empty\n",
    "if not os.listdir(INPUT_DATA_DIR):\n",
    "    print(\"Downloading Middlebury dataset\")\n",
    "    !wget https://vision.middlebury.edu/stereo/data/scenes2021/zip/all.zip -O all.zip\n",
    "    !unzip -q all.zip -d {INPUT_DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download checkpoint and config. If the model is too big for you GPU, you can use a smaller one, e.g. sam2.1_hiera_base_plus.pt + sam2.1_hiera_base+.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.listdir(CHECKPOINT_PATH):\n",
    "    !wget https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt -O {CHECKPOINT_PATH}/sam2.1_hiera_large.pt\n",
    "    !wget https://raw.githubusercontent.com/facebookresearch/sam2/refs/heads/main/sam2/configs/sam2.1/sam2.1_hiera_l.yaml -O {CHECKPOINT_PATH}/sam2.1_hiera_l.yaml\n",
    "sam2_checkpoint = Path(CHECKPOINT_PATH) / \"sam2.1_hiera_large.pt\"\n",
    "sam2_config = Path(CHECKPOINT_PATH) / (\"sam2.1_hiera_l.yaml\")\n",
    "# Don't ask why \"/\" is needed in front of sam2_config, it's just how it works\n",
    "sam2_config_full_path = \"/\"+str(sam2_config.absolute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate mask generator. The parameters here can be tweaked, but the default ones should work fine. The most important one here may be the min_mask_region_area, which is (surprisingly) the minimum area of the mask region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_predictor = build_sam2(sam2_config_full_path, sam2_checkpoint.absolute(), device=device, apply_postprocessing=False)\n",
    "\n",
    "mask_generator= SAM2AutomaticMaskGenerator(\n",
    "    model=image_predictor,\n",
    "    points_per_side=32,\n",
    "    points_per_batch=64,\n",
    "    pred_iou_thresh=0.5,\n",
    "    stability_score_thresh=0.92,\n",
    "    stability_score_offset=0.7,\n",
    "    crop_n_layers=0,\n",
    "    box_nms_thresh=0.5,\n",
    "    crop_n_points_downscale_factor=2,\n",
    "    min_mask_region_area=250.0,\n",
    "    use_m2m=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image0 = Image.open(input_dir / 'im0.png')\n",
    "image0 = np.array(image0.convert(\"RGB\"))\n",
    "image1 = Image.open(input_dir / 'im1.png')\n",
    "image1 = np.array(image1.convert(\"RGB\"))\n",
    "\n",
    "\n",
    "masks0 = mask_generator.generate(image0)\n",
    "# masks1 = mask_generator.generate(image1)\n",
    "\n",
    "save_annotations(masks0, path=OUT_PATH + \"0.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the masks and their IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of masks found: {len(masks0)}\")\n",
    "\n",
    "# convert segmentation to mask logits\n",
    "\n",
    "masks = []\n",
    "mask_ids = []\n",
    "for i, mask in enumerate(masks0):\n",
    "    masks.append(mask[\"segmentation\"])\n",
    "    mask_ids.append(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up some memory to avoid OOM from CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del mask_generator\n",
    "del image_predictor\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SEGMENTATION\n",
    "\n",
    "Instantiate video predictor - treat two images as a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_predictor = build_sam2_video_predictor(sam2_config_full_path, sam2_checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import show_box, show_mask, show_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the frames and change png to jpg because the video predictor expects jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_names = [\n",
    "    p for p in os.listdir(input_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".png\", \".PNG\"]\n",
    "]\n",
    "\n",
    "frames_path = Path(input_dir) / \"frames\"\n",
    "frames_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for frame_name in frame_names:\n",
    "    img = Image.open(os.path.join(input_dir, frame_name))\n",
    "    img.save(os.path.join(frames_path, frame_name.replace('im','').replace('.png', '.jpeg')))\n",
    "\n",
    "frame_names = [x.replace('im','').replace('png','jpeg') for x in frame_names if 'im' in x]\n",
    "frame_names.sort(key=lambda p: os.path.splitext(p)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SEGMENT-LEVEL STEREO MATCHING\n",
    "\n",
    "Get the masks and their IDs from the first image and propagate them to the second image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state = video_predictor.init_state(video_path=str(frames_path))\n",
    "MASK_LIMIT = 45\n",
    "\n",
    "for mask_id, mask in zip(mask_ids, masks):\n",
    "    if mask_id<MASK_LIMIT:\n",
    "        video_predictor.add_new_mask(inference_state, 0, mask_id, mask)\n",
    "\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in video_predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 1\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(frames_path, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load reference disparity maps - this will be our ground truth used to evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pfm depth data from disp0.pfm and disp1.pfm\n",
    "\n",
    "disp0 = np.array(Image.open(Path(input_dir) / 'disp0.pfm'))\n",
    "disp1 = np.array(Image.open(Path(input_dir) / 'disp1.pfm'))\n",
    "\n",
    "# show the images\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"disp0\")\n",
    "plt.imshow(disp0, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"disp1\")\n",
    "plt.imshow(disp1, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PIXEL-LEVEL STEREO MATCHING\n",
    "\n",
    "Now, for the sake of the demonstration, just a single mask will be selected. In the real application, all masks would be used.\n",
    "\n",
    "**Note**: The same mask ID is used for both images. The ID consistency between images is caused by the propagation of the masks from the first image to the second image (done above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_MASK_ID = 20\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.title(f\"mask {SELECTED_MASK_ID} - frame left\")\n",
    "mask0 = masks[SELECTED_MASK_ID]\n",
    "plt.imshow(image0)\n",
    "show_mask(mask0, plt.gca(), obj_id=SELECTED_MASK_ID, random_color=False)\n",
    "plt.show()\n",
    "\n",
    "plt.title(f\"mask {SELECTED_MASK_ID} - frame right\")\n",
    "plt.imshow(image1)\n",
    "mask1 = video_segments[1][SELECTED_MASK_ID][0]\n",
    "show_mask(mask1, plt.gca(), obj_id=SELECTED_MASK_ID, random_color=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally remove small blobs from the masks (happens sometimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "\n",
    "def remove_small_blobs(mask, min_size=100):\n",
    "    labels = measure.label(mask)\n",
    "    mask = np.zeros_like(mask)\n",
    "    for region in measure.regionprops(labels):\n",
    "        if region.area >= min_size:\n",
    "            mask[labels == region.label] = 1\n",
    "    return mask\n",
    "\n",
    "mask0 = remove_small_blobs(mask0, min_size=100)\n",
    "mask1 = remove_small_blobs(mask1, min_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions to cut out the mask from the image using the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find bounding boxes for masks in left and right images\n",
    "\n",
    "from helpers import find_bbox, crop_image\n",
    "\n",
    "bbox_left = find_bbox(mask0)\n",
    "bbox_right = find_bbox(mask1)\n",
    "\n",
    "# crop the bounding boxes from the images\n",
    "\n",
    "crop0 = crop_image(image0, bbox_left)\n",
    "crop1 = crop_image(image1, bbox_right)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Crop left\")\n",
    "plt.imshow(crop0)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Crop right\")\n",
    "plt.imshow(crop1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge bboxes to have the same size of the cutout images and calculate the offset for the disparity. The offset is a constant value that is added to the disparity to get the disparity in the original image. Having the same size of the cutout images makes it easier to calculate the disparity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import crop_and_align_masks\n",
    "\n",
    "# It's basically the same crop as above, but with the offset calculated, which is critical for 3D reconstruction\n",
    "image0_crop_aligned, image1_crop_aligned, mask0_crop_aligned, mask1_crop_aligned, offset = crop_and_align_masks(image0, image1, mask0, mask1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image left cropped\")\n",
    "plt.imshow(image0_crop_aligned)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"image right cropped\")\n",
    "plt.imshow(image1_crop_aligned)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can probably be done with some other algorithm or approach here, but the point is that the sum of absolute differences is calculated for each pair of blocks (of the same size) in the left and right images. It is necessary to know where the difference is the lowest to find the match for each source pixel. The algorithm is quite simple and can be optimized in many ways. It doesn't check other heights because we assume that the images are stereo rectified (correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 21\n",
    "\n",
    "from algo import calculate_sad_values\n",
    "\n",
    "# Convert images to np.float32\n",
    "image0_crop_aligned = image0_crop_aligned.astype(np.float32)\n",
    "image1_crop_aligned = image1_crop_aligned.astype(np.float32)\n",
    "\n",
    "\n",
    "sads = calculate_sad_values(image0_crop_aligned, image1_crop_aligned, mask0_crop_aligned, mask1_crop_aligned, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the shape of SAD is (image0_height, image0_width, image1_width), meaning that SADs are stored for each row, then for each pixel in the left image, and finally for each pixel in the right image. The \"pixel\" means the central pixel of a block.\n",
    "\n",
    "Now, visualize the best matches for each pixel in the left image. The best matches are the ones with the lowest SAD. Ideally, the image should be smooth, but for a uniform texture, it is hard to find the correct match. Here's a visualization for a \"naive\" approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Matched pixels (at the same height) for each pixel in the left image\")\n",
    "plt.imshow(sads.argmin(axis=2), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bunch of additional visualizations\n",
    "\n",
    "1. The disparity map for a given height with the best matches - for a narrower \"paths\" it's easier for fidn a match. Bigger blobs of uniform color denote regions that are visually similar, and probably kind of interpolation should be used there.\n",
    "2. Visualizaiton of the compared slices, to see what is actually compared\n",
    "3. The minimal SAD value for each pixel in the left image - theoretically, the lower the value, the better the match\n",
    "4. The best matching argument for each pixel on a given height in the left image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZATION_INDEX = 82 # make sure it's not exceeding the height of the mask\n",
    "\n",
    "# show sad and mark minimal values in each row with red\n",
    "\n",
    "from helpers import find_left_right_bounds_for_all_heights\n",
    "\n",
    "left_bounds0, right_bounds0 = find_left_right_bounds_for_all_heights(mask0_crop_aligned)\n",
    "left_bounds1, right_bounds1 = find_left_right_bounds_for_all_heights(mask1_crop_aligned)\n",
    "\n",
    "exemplary_sad = sads[VISUALIZATION_INDEX,left_bounds0[VISUALIZATION_INDEX]:right_bounds0[VISUALIZATION_INDEX],left_bounds1[VISUALIZATION_INDEX]:right_bounds1[VISUALIZATION_INDEX]]\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(f\"Normalized Sum of Absolute Differences, height={VISUALIZATION_INDEX}\")\n",
    "plt.imshow(exemplary_sad, cmap='jet')\n",
    "plt.xlabel(\"right image x\")\n",
    "plt.ylabel(\"left image x\")\n",
    "\n",
    "argmin_sad = np.argmin(exemplary_sad, axis=1)\n",
    "min_sad = np.min(exemplary_sad, axis=1)\n",
    "\n",
    "# draw a line going through the diagonal of the plot\n",
    "plt.plot(argmin_sad, np.array(range(0, exemplary_sad.shape[0])) , color='red')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# show slice of left image with height denoted by window_size and right image below\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(\"left image slice\")\n",
    "plt.imshow(image0_crop_aligned[VISUALIZATION_INDEX-WINDOW_SIZE//2:VISUALIZATION_INDEX+WINDOW_SIZE//2+1,  left_bounds0[VISUALIZATION_INDEX]:right_bounds0[VISUALIZATION_INDEX]].astype(np.uint8))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"right image slice\")\n",
    "plt.imshow(image1_crop_aligned[VISUALIZATION_INDEX-WINDOW_SIZE//2:VISUALIZATION_INDEX+WINDOW_SIZE//2+1,  left_bounds1[VISUALIZATION_INDEX]:right_bounds1[VISUALIZATION_INDEX]].astype(np.uint8))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# show the minimal sad values\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title(f\"Minimal SAD values, height={VISUALIZATION_INDEX}\")\n",
    "plt.plot(min_sad.T)\n",
    "plt.xlabel(\"left image x\")\n",
    "plt.ylabel(\"minimal SAD\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# show the minimal sad arguments\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title(f\"Arg for Minimal SAD values, height={VISUALIZATION_INDEX}\")\n",
    "plt.plot(argmin_sad.T)\n",
    "plt.xlabel(\"left image x\")\n",
    "plt.ylabel(\"minimal SAD\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get even deeper insights, visualize exemplary SAD values for a given height for a given left pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_INDEX = 20\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title(f\"Minimal SAD values, height={VISUALIZATION_INDEX}\")\n",
    "plt.plot(exemplary_sad[:, LEFT_INDEX], 'x')\n",
    "plt.xlabel(\"right image x\")\n",
    "plt.ylabel(\"minimal SAD\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. CONFIDENCE REFINEMENT\n",
    "\n",
    "Now, the confidence is calculated for each match. The confidence is calculated to retain only the high-confidence matches. The confidence is calculated as the ratio of the minimum SAD to the second minimum SAD. The confidence is then used to filter out the matches with low confidence.\n",
    "\n",
    "The current confidence estimation algorithm works as follows:\n",
    "1. For each pixel in the left image, the SADs are calculated for each pixel in the right image.\n",
    "2. For a given pixel in the left image:\n",
    "    - The minimal SAD is found\n",
    "    - The angles of a line (red line in the figure) between the minimal SAD location and any other location is calculated\n",
    "    - The confidence is the minimal angle found for a given pixel in the left image\n",
    "\n",
    "The reasoning of the above is that the directly surrounding pixels will naturally be more similar to each other, especially for uniform texture patches.\n",
    "\n",
    "The algorithm strongly relies on the confidence calculated. Therefore, there's room for improvement and any other function can be used. One way to improve it is to take relative distance between the left and right pixels into account, or ensure continuity of the disparity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEMPLARY_LEFT_INDEX = 10\n",
    "# find the line that goes from the minimal sad value and is under the minimal sad values\n",
    "\n",
    "index_sad = exemplary_sad[EXEMPLARY_LEFT_INDEX, :]\n",
    "\n",
    "# find the minimal sad value\n",
    "min_sad_value = index_sad.min()\n",
    "\n",
    "# find the minimal sad value index\n",
    "min_sad_index = index_sad.argmin()\n",
    "print(min_sad_index)\n",
    "\n",
    "# for each point not being the minimum in index_sad draw a line from minium to that point\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Minimal SAD values\")\n",
    "plt.plot(index_sad, 'x')\n",
    "plt.xlabel(\"right image x\")\n",
    "plt.ylabel(\"minimal SAD\")\n",
    "for i, sadd in enumerate(index_sad):\n",
    "    if i != min_sad_index:\n",
    "        plt.plot([min_sad_index, i], [min_sad_value, sadd], color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the angles of each red line are calculated and shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "angles = np.abs(np.arctan2(np.array(range(0, exemplary_sad.shape[1])) - min_sad_index, min_sad_value - index_sad))\n",
    "\n",
    "# print the second minimal angle\n",
    "print(f\"Minimal angle: {angles[angles != 0].min():.4f} for right-image pixel {angles[angles != 0].argmin()}\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title(f\"Angles for exemplary left image pixel with height {VISUALIZATION_INDEX} and x coordinate {EXEMPLARY_LEFT_INDEX}\")\n",
    "plt.plot(angles)\n",
    "plt.xlabel(\"right image x\")\n",
    "plt.ylabel(\"angle\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code for the confidence calculation. The confidence is calculated for each pixel in the left image. The confidence heatmap is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algo import find_confidence\n",
    "\n",
    "# TODO: add this part to the find_confidence function\n",
    "confidences = np.zeros((sads.shape[0], sads.shape[1]))\n",
    "for j in range(sads.shape[0]):\n",
    "    if left_bounds0[j] == right_bounds0[j] or left_bounds1[j] == right_bounds1[j]:\n",
    "        continue\n",
    "    sad = sads[j, left_bounds0[j]:right_bounds0[j], left_bounds1[j]:right_bounds1[j]]\n",
    "    for i in range(sad.shape[0]):\n",
    "        confidences[j,i+left_bounds0[j]] = find_confidence(sad, i)\n",
    "\n",
    "# plot confidences as heatmap\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Matching confidence map\")\n",
    "plt.imshow(confidences, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Left image width\")\n",
    "plt.ylabel(\"Left image height\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the argument of the minimal value in sads\n",
    "\n",
    "argmin_sads= np.argmin(np.nan_to_num(sads, nan=float('inf')), axis=2)\n",
    "\n",
    "print(argmin_sads.shape)\n",
    "# subtract the location of x from argmin_sads\n",
    "\n",
    "matches = argmin_sads.astype(np.float64)\n",
    "# put nan where the mask is not present\n",
    "matches[mask0_crop_aligned == 0] = np.nan\n",
    "disparities = (argmin_sads - np.array(range(0, sads.shape[2]))[np.newaxis, np.newaxis, :])[0]\n",
    "print(disparities.shape)\n",
    "\n",
    "# find maximal value of confidences ignoring nan values\n",
    "\n",
    "max_confidence = np.nanmax(confidences)\n",
    "print(max_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold = 2\n",
    "thresholded_confidences = confidences.copy()\n",
    "thresholded_confidences[thresholded_confidences < threshold] = 0\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Matching confidence map\")\n",
    "plt.imshow(thresholded_confidences, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Left image width\")\n",
    "plt.ylabel(\"Left image height\")\n",
    "plt.show()\n",
    "\n",
    "# draw argmin_sads where the confidence is non-zero and non-nan\n",
    "\n",
    "disparities_left = disparities.copy().astype(np.float64) # - offset\n",
    "disparities_left[thresholded_confidences == 0] = np.nan\n",
    "\n",
    "# put nan where the confidence is nan\n",
    "\n",
    "\n",
    "\n",
    "def get_confidences(sad_matrix, left_bounds0, right_bounds0, left_bounds1, right_bounds1):\n",
    "    confidences = np.zeros((sad_matrix.shape[0], sad_matrix.shape[1]))\n",
    "    for j in range(sad_matrix.shape[0]):\n",
    "        if left_bounds0[j] == right_bounds0[j] or left_bounds1[j] == right_bounds1[j]:\n",
    "            continue\n",
    "        sad = sad_matrix[j, left_bounds0[j]:right_bounds0[j], left_bounds1[j]:right_bounds1[j]]\n",
    "        for i in range(sad.shape[0]):\n",
    "            confidences[j,i+left_bounds0[j]] = find_confidence(sad, i)\n",
    "    return confidences\n",
    "\n",
    "confidences = get_confidences(sads, left_bounds0, right_bounds0, left_bounds1, right_bounds1)\n",
    "\n",
    "disparities_left[np.isnan(confidences)] = 0\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Disparities\")\n",
    "plt.imshow(disparities_left, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Left image width\")\n",
    "plt.ylabel(\"Left image height\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "argmin_sads= np.argmin(np.nan_to_num(sads, nan=float('inf')), axis=2)\n",
    "\n",
    "print(argmin_sads.shape)\n",
    "# subtract the location of x from argmin_sads\n",
    "\n",
    "matches = argmin_sads.astype(np.float64)\n",
    "matches_filled = np.ones_like(matches)*-1\n",
    "matches_filled[mask0_crop_aligned == 0] = np.nan\n",
    "matches_filled_0 = np.ones_like(matches)*-1\n",
    "matches_filled_0[mask0_crop_aligned == 0] = np.nan\n",
    "sads_c = sads.copy()\n",
    "confidences_c = get_confidences(sads_c, left_bounds0, right_bounds0, left_bounds1, right_bounds1)\n",
    "\n",
    "threshold = 2.1\n",
    "\n",
    "\n",
    "matches_filled_0[confidences_c > threshold] = matches[confidences_c > threshold]\n",
    "# copy the matches to matches_filled where the confidence is above the threshold\n",
    "matches_filled[confidences_c > threshold] = matches[confidences_c > threshold]\n",
    "\n",
    "@dataclass\n",
    "class Range:\n",
    "    start: int\n",
    "    end: int\n",
    "    min_val: int\n",
    "    max_val: int\n",
    "\n",
    "def get_ranges(non_nan, valid, matches_row):\n",
    "    # TODO: adde a range in case of [... 135, 135, -1, -1, nan, nan, 135, nan, ...]\n",
    "    ranges = []\n",
    "    curr_left = valid[0]\n",
    "    curr_right = valid[1]\n",
    "    if non_nan[0] != curr_left:\n",
    "        ranges.append(Range(non_nan[0], curr_left, 0, matches_row[valid[0]]))\n",
    "    for right_idx in valid[2:]:\n",
    "        if curr_right > curr_left + 1:\n",
    "            ranges.append(Range(curr_left, curr_right, matches_row[curr_left], matches_row[curr_right]))\n",
    "        curr_left = curr_right\n",
    "        curr_right = right_idx\n",
    "    return ranges\n",
    "\n",
    "# for each row\n",
    "for j in range(100):\n",
    "    for i in range(matches_filled.shape[0]):\n",
    "        # find the first non-nan value\n",
    "        non_nan_values = np.where(~np.isnan(matches_filled[i]))[0]\n",
    "        valid_values = np.where(~np.isnan(matches_filled[i]) * matches_filled[i]>-1)[0]\n",
    "        if len(non_nan_values) == 0:\n",
    "            continue\n",
    "        if len(valid_values) == len(non_nan_values):\n",
    "            # nothing to fill, row is already filled\n",
    "            continue\n",
    "        first_non_nan = non_nan_values[0]\n",
    "        last_non_nan = non_nan_values[-1]\n",
    "        # fill the values before the first non-nan value with the first non-nan value\n",
    "        if len(valid_values) > 1:\n",
    "            first_valid = valid_values[0]\n",
    "            last_valid = valid_values[-1]\n",
    "            ranges_to_check = get_ranges(non_nan_values, valid_values, matches_filled[i])\n",
    "            for range_to_check in ranges_to_check:\n",
    "                if i==0:\n",
    "                    print(range_to_check)\n",
    "\n",
    "                sads_c[i,range_to_check.start:range_to_check.end+1,:np.int64(np.floor(range_to_check.min_val))]=np.inf\n",
    "                sads_c[i,range_to_check.start:range_to_check.end+1,np.int64(np.ceil(range_to_check.max_val))+1:]=np.inf\n",
    "    confidences_c = get_confidences(sads_c, left_bounds0, right_bounds0, left_bounds1, right_bounds1)\n",
    "    # additions to confidences where neighbouring pixels are distant by a distance_threshold\n",
    "    distance_threshold = 2.5\n",
    "    conf_increment = 0.3\n",
    "    for i in range(1, confidences_c.shape[1]-1):\n",
    "        for j in range(1, confidences_c.shape[0]-1):\n",
    "            if np.abs(matches_filled[j, i-1] - matches[j, i]) < distance_threshold:\n",
    "                confidences_c[j, i] += conf_increment\n",
    "            if np.abs(matches_filled[j, i+1] - matches[j, i]) < distance_threshold:\n",
    "                confidences_c[j, i] += conf_increment\n",
    "            if np.abs(matches_filled[j-1, i] - matches[j, i]) < distance_threshold:\n",
    "                confidences_c[j, i] += conf_increment\n",
    "            if np.abs(matches_filled[j+1, i] - matches[j, i]) < distance_threshold:\n",
    "                confidences_c[j, i] += conf_increment\n",
    "\n",
    "    threshold = threshold - 0.02\n",
    "\n",
    "    argmin_sads= np.argmin(np.nan_to_num(sads_c, nan=float('inf')), axis=2)\n",
    "\n",
    "    matches = argmin_sads.astype(np.float64)\n",
    "    matches_filled[(confidences_c > threshold) * (matches_filled<0)] = matches[(confidences_c > threshold) * (matches_filled<0)]\n",
    "print(matches_filled_0[30])\n",
    "print(matches_filled[30])\n",
    "\n",
    "print(confidences_c[30])\n",
    "# show matches filled\n",
    "\n",
    "plt.figure(figsize=(18, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Matches - before filling\")\n",
    "plt.imshow(matches_filled_0, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Left image width\")\n",
    "plt.ylabel(\"Left image height\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Matches filled\")\n",
    "plt.imshow(matches_filled, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Left image width\")\n",
    "plt.ylabel(\"Left image height\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change matches to disparities\n",
    "\n",
    "disparities_left = (matches_filled - np.array(range(0, sads.shape[2]))[np.newaxis, np.newaxis, :])[0]\n",
    "disparities_left[mask0_crop_aligned == 0] = np.nan\n",
    "\n",
    "# add offset\n",
    "print(offset)\n",
    "disparities_left -= offset\n",
    "\n",
    "disparities_left = np.abs(disparities_left)\n",
    "\n",
    "\n",
    "# show the filled disparities\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Disparities\")\n",
    "plt.imshow(disparities_left, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Left image width\")\n",
    "plt.ylabel(\"Left image height\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. GLOBAL REFINEMENT\n",
    "\n",
    "Now the part 5 should follow, but it's not implemented yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. EVALUATION\n",
    "\n",
    "The evaluation is done using the Middlebury Stereo Dataset 2021. The disparities are calculated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the disp0.pfm and disp1.pfm files\n",
    "\n",
    "disp0 = np.array(Image.open(Path(input_dir) / 'disp0.pfm'))\n",
    "disp1 = np.array(Image.open(Path(input_dir) / 'disp1.pfm'))\n",
    "\n",
    "# show the images\n",
    "\n",
    "plt.figure(figsize=(20, 9))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"disp0\")\n",
    "plt.imshow(disp0, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"disp1\")\n",
    "plt.imshow(disp1, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop the disparities to the analyzed mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask and then crop the image\n",
    "\n",
    "\n",
    "disp0ca, disp1ca, mask0_crop_aligned, mask1_crop_aligned, offset = crop_and_align_masks(disp0, disp1, mask0, mask1)\n",
    "\n",
    "# show side-by-side disp0ca and disparities_left\n",
    "\n",
    "disparities_leftl = np.abs(disparities_left.copy())\n",
    "disparities_leftl[disparities_leftl > 90] = 0\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"disp0ca\")\n",
    "plt.imshow(disp0ca, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"disparities_left\")\n",
    "plt.imshow(np.abs(disparities_leftl), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the evaluation metrics\n",
    "\n",
    "# for existing disparities\n",
    "\n",
    "indices = disparities_leftl > 0 & ~np.isinf(disparities_leftl)\n",
    "indices = indices & ~np.isinf(disp0ca)\n",
    "\n",
    "print(disparities_leftl[indices])\n",
    "print(disp0ca[indices])\n",
    "# calculate the mean absolute error\n",
    "\n",
    "mae = np.mean(np.abs(disparities_leftl[indices] - disp0ca[indices]))\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sastma-core-Jn3E1VNw-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
